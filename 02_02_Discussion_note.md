# ðŸŒ‹ 2nd meeting : Natural Language Processing with Disaster Tweets 

**written by Soyoun Son**         
**Date : 050222**

#### ðŸ¦† Kaggle : https://www.kaggle.com/c/nlp-getting-started

## ðŸŒ± í’€ìžŽìŠ¤ì¿¨ (5ì›” 2ì¼) 

ì„œë¡œ ê°™ì´ ë…¼ì˜ëœ ë¶€ë¶„ ë° ê³µë¶€í•œ ê²ƒì— ëŒ€í•´ì„œ ì ìŒ

### â˜ºï¸Ž Content
- [x] NLP preprocess
- [x] ê°œë… ì •ë¦¬ : TF-IDF
- [x] 10 steps for challenging and improving NLP model
- [x] Discussion


### â˜ºï¸Ž [NLP preprocess](https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z/notebook) 

+ read and explore data 
+ text cleaning 

| capitalization | expand the contractions | remove noise, punctuations | íŠ¹ìˆ˜ë¬¸ìžëŠ” characterë¡œ ë°”ê¿ˆ | 
| -------------- | ----------------------- | -------------------------- | ----------------------- |
| ëŒ€ë¬¸ìž-> ì†Œë¬¸ìž |  |   |   |


> í•œêµ­ì–´ë¡œ ìž¡ì—…í• ë•ŒëŠ” capitalization, expand the contractionsë¶€ë¶„ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ      

+ text pre-processing 
  - tokenization 
  - remove stop words : nltkì— tokenizer ì‚¬ì „ì´ ì¡´ìž¬. í•œêµ­ì–´ì˜ ê²½ìš°ëŠ” POS tagging ì´í›„ì— ì§„í–‰
  - stemming : ì–´ê°„ì¶”ì¶œ, ë¶„ì„ê²°ê³¼ê°€ ì•ˆì¢‹ì•˜ì„ë•Œ í•˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì  
  - POS tagging : í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ. ë™ì‚¬/ëª…ì‚¬/... ë¬¸ë§¥ì ìœ¼ë¡œ ìž˜ íŒŒì•…í• ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ê³¼ì •. í’ˆì‚¬ì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë°œìƒ. í•œêµ­ì–´ì˜ ê²½ìš° KoNLPy
  - Lemmatization : í‘œì œì–´ ì¶”ì¶œ. ê¸°ë³¸í˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì •. ë”°ë¼ì„œ í’ˆì‚¬ ì§€ì •í•˜ëŠ” PoS taggingí•œ í›„ì— ì§„í–‰í•´ì•¼ í•¨. 
  - (optional) language detection 

> í•œêµ­ì–´ì˜ ê²½ìš° í† í°í™”-> POS tagging -> stemming/lemmatization -> remove stop words         
í•œêµ­ì–´ì˜ ê²½ìš° í‘œì œì–´ ì¶”ì¶œ, ì–´ê°„ì¶”ì¶œì˜ ì°¨ì´ê°€ ë§Žì´ ì—†ê³ , ì–´ê°„ì¶”ì¶œì´ ê·¸ë‚˜ë§ˆ ë” ë§Žì´ ì‚¬ìš©ë¨

+ Text Feature Extraction 
  + weighted words - BOW
    - countvectorizer
    - TF-IDF

> í•œê³„ : ì¹´ìš´íŒ…í•˜ì§€ë§Œ, ìœ„ì¹˜ì •ë³´ë¥¼ ìžƒìŒ. ê·¸ëž˜ì„œ ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•… ë¶ˆê°€      

+ word embedding 
  - word2vec -> gensim 
  - Glove
  - FastText
  - Bert : transformer 30ì–µê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í•™ìŠµì‹œí‚¨í›„, ë³¸ì¸ì˜ í…ŒìŠ¤í¬ì— ë”°ë¼ì„œ fine tunningì„ í•˜ë©´ ì •í™•ë„ê°€ ë†’ìŒ. 

+ comparison of feature extraction techinique  

ì˜ˆë¡œ Bertì˜ í•œê³„ê°€ ì¡´ìž¬í•œë‹¤ë©´, 
ì´ëŸ° ë¬¸ì œì˜€ì„ë•ŒëŠ” ë‹¤ë¥¸ ê²ƒ ì‚¬ìš© í•  ê²ƒ
ë‹¤ë¥¸ ì ‘ê·¼ë²•: ê° ë°©ë²•ì˜ ì•½ì ì„ í™•ì¸í•˜ë©´ ì¢‹ì„ ë“¯
Bert (Transformer)ë§Œ ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ RNN
A : architecture 


### â˜ºï¸Ž [ê°œë… ì •ë¦¬ : TF-IDF](https://www.notion.so/modulabs/NLP-w-DL-061fbb36c67d494fa062309914b4842d?p=f0a9b205d22545fe9e0707b8493f57ac) 
+ ì „ì²˜ë¦¬ ìž‘ì—…
- ê¸ì •ë¬¸ì„œ/ë¶€ì •ë¬¸ì„œ
- tf-idf 
- ë“±ìž¥ í‚¤ì›Œë“œì— ëŒ€í•œ scoring 
+ ëª¨ë¸ 
- decision tree, regression 
> ì‹¤ë¬´ ì ìš©ì‹œ : ìž¬ë‚œì˜ ìœ ë¬´ê°€ ì•„ë‹ˆë¼ ìž¬ë‚œì˜ ì¹´í…Œê³ ë¦¬ê¹Œì§€ ì°¾ì•„ì•¼ í•œë‹¤.

### â˜ºï¸Ž [10 steps for challenging and improving NLP model](https://www.notion.so/modulabs/NLP-w-DL-061fbb36c67d494fa062309914b4842d?p=bc0c6c7c725246f9a2920c87fb2a6af3)

3. one-hot encoding : GPTê°™ì€ ê²ƒì€ ë¼ë²¨ì´ onehot encodingìœ¼ë¡œ ë‚˜ì˜´         
4. ê¸¸ì´ê°€ ë“¤ì­‰ë‚ ì­‰í•œê²ƒ ë³´ë‹¤ íŒ¨ë”©ìžˆëŠ” ê²ƒì´ ê²°ê³¼ê°€ ì¢‹ì•˜ë‹¤?       
5. ì»¤ë¸Œê°€ ë‚´ë ¤ê°€ë‹¤ê°€ ë›°ë©´ ì˜¤ë²„í”¼íŒ…í•˜ëŠ” ê²ƒì´ê³ , ê³„ì† ë‚´ë ¤ê°€ë©´ ìƒê´€ì—†ìŒ        
shuffle : ë°ì´í„°ê°€ ë­”ê°€ ì—°ì†ì ìœ¼ë¡œ ë¹„ìŠ·í•œ í˜•íƒœë¼ë©´ ê·¸ê²ƒì„ ë§‰ê¸°ìœ„í•´ì„œ ì„žëŠ” ê²ƒ       
10. scale factors 

### â˜ºï¸Ž Discussion
+ ì‹¤ë¬´ì— ì ìš©í•œë‹¤ë©´, 
  - ê¸ì •/ë¶€ì •ë¼ë¦¬ ëª¨ìœ¼ëŠ” ì „ì²˜ë¦¬ : ì‚¬ëžŒì´ annotationì´ ë˜ì•¼í•¨.
  - TF-IDFë§Œ ì‚¬ìš©í•œë‹¤ë©´ 2ë²ˆì„ íƒœì›Œì•¼ í•  ê²ƒ
  - í‚¤ì›Œë“œ ì§€ì‹ : ì‚¬ì „êµ¬ì¶•, ì‹¤ì œ ìžì—°ì–´ì²˜ë¦¬ë¥¼ í• ë•Œ, íŠ¹ì • ë„ë©”ì¸ì— ë§žëŠ” ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ. ì€í–‰ ê´€ë ¨ ë„ë©”ì¸ì˜ í‚¤ì›Œë“œëŠ” 'ê³„ì¢Œì´ì²´'ì´ëŸ° ê²ƒ ë“¤
  - í‚¤ì›Œë“œ ì§€ì‹ì´ ì´ë¯¸ ê°–ì¶°ì§„ pre-trained modelì´ ìžˆë‹¤ë©´ 

+ ê³µê³µë„ë©”ì¸ ë°ì´í„°ì˜ ê²½ìš°, í‘œì¤€í™”ê°€ ëœ ë°ì´í„°ì´ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ë¥¼ ë°›ì•˜ì„ë•ŒëŠ” ì¤„ì´ëŠ” ê²ƒì´ í•„ìš”
+ TF-IDFëŒ€ì‹  countvectorì´ìš©í•´ì„œ TFë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì„ë“¯ 
+ Bertì— ìµœì í™”ë˜ì–´ ìžˆëŠ” ê²ƒ 
+ íŒŒì´í† ì¹˜ì˜ ê²½ìš° ì²«ë²ˆì§¸ ì—í¬í¬ì—ì„œ ì´ë¯¸ ê²°ì •ì´ ë‚¨ 
++++++++++



****** ì‹¤ì œ ì‹¤ë¬´ì—ì„œ í™•ì¸ í•  ê²ƒ ***
ëª¨ë¸ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„°ì—ì„œ ìŠ¹ë¶€ê°€ ë‚˜ëŠ” ê²ƒ 
ì°¨ë¼ë¦¬ í† í¬ë„ˆë‚˜ì´ì €ê°™ì€ ê²ƒì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ 
ì—­ì‚¼ê°í˜•ì´ì–´ì„œ ëª¨ë¸ì„ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤, ì´ˆê¸°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì¢‹ìŒ 
ë²„íŠ¸ë„ ë²„íŠ¸ì˜ ëª¨ë¸ë³´ë‹¤ oovì¤„ì—¬ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒ
í† í¬ë‚˜ì´ì €ì—ì„œ ì›Œë“œ í”¼ìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒ. 
íŒŒì‹±í• ë•Œ (í•œêµ­ì–´ì—ì„œ ì–´ê°„, ì–´ë¯¸ ìª¼ê°¤ë•Œ)ì™€ ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì¼ì„ë•Œ ì„±ëŠ¥ ì˜¬ë¼ê°
ì—¬ì „ížˆ ë„ë©”ì¸ì´ ì¤‘ìš”. ë„ë©”ì¸ì—ì„œ ë§Žì´ ì“°ëŠ” ì–¸ì–´ê°€ ë‹¨ì–´ë¡œ í• ë‹¹ë¨?


### kaggle notebookì„ githubìœ¼ë¡œ ê°–ê³  ì˜¤ëŠ” ê²½ìš° 
https://www.kaggle.com/product-feedback/295170

https://somjang.tistory.com/category/Kaggle/Real%20or%20Not%3F%20NLP%20with%20Disaster%20Tweets


ëª©ì  ìµœì í™”ëœ ëª¨ë¸ ì°¾ëŠ”ê²ƒ?
Gridserch CVë¡œ hyperparameter?
ì „ì²˜ë¦¬ ìž‘ì—…ì„ ì§„í–‰ì•ˆí•˜ê³ 
ì „ì²˜ë¦¬ìž‘ì—…í•˜ê³  ì§„í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì „ì²˜ë¦¬ë¥¼ ë’¤ì—ì„œ í™•ì¸í•˜ëŠ” ê²ƒ?
ë¹„êµí•˜ëŠ” ê²ƒì€ accuracy?
score? 20%ì— ì–´ë–¤ê²ƒë“¤ì´ í‹€ë ¸ëŠ”ì§€? ê·¸ê²ƒì„ í™•ì¸í•˜ëŠ” ê²ƒ?
ëª¨ë¸ì— ë”°ë¼ì„œ ëª» ë§žì¶”ëŠ” ê²ƒë“¤ì´ ë‹¤ë¥´ì§€ ì•Šì„ê¹Œ? 
epoch 1-5 : ì„±ëŠ¥ì€ ì•žë‹¨ì—ì„œ ê²°ì •ëœë‹¤ëŠ” ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ìƒê°í•œê±´ê°€? 

hyperparatmer íŠœë‹ì„ AutoMLì´ ë‹¤í•´ì¤Œ?

ë” ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ê³  ìžˆë‹¤ê³  í•´ì„œ ì¢‹ì€ ëª¨ë¸ì´ë¼ê³  ë³¼ìˆ˜ ìžˆëŠ”ê°€? 
ëª¨ì§‘ë‹¨ì—ì„œ ìƒ˜í”Œì„ ë½‘ì•„ì„œ í™•ì¸í• ë•Œ, ê·¸ ìƒ˜í”Œì´ ëª¨ì§‘ë‹¨ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ê³  ìžˆëŠ” ê²ƒì¼ê¹Œ? 

ì¢‹ì€ ì„±ì  ì–»ê¸° ìœ„í•œ ëª¨ë¸ [ref](https://github.com/MLFS19-NLP/KaggleNotebooks/blob/main/nlp-a-gentle-introduction-lstm-word2vec-bert.ipynb)
LSTM, simple RNN, word2Vec w/ Gensim, BERT,Voting
-> ì˜¤ë‹µì„ ë´ì„œ (ì ìˆ˜ê°€ ë¹„ìŠ·í•˜ë”ë¼ë„) 

ì˜¤ë‹µì´ ëª°ë ¤ìž‡ë‹¤í•˜ë©´ í•©ì¹˜ë©´
ì—‘ìŠ¤ì¥ë¶€ìŠ¤íŠ¸ : ì˜¤ë‹µì„ ì¤„ì´ëŠ” ëª¨ë¸, ëŒ€ì‹  ê³¼ì í•© ìž˜ë¨.

LSTM : ë§¨ ë’¤ì—
BERT, simpleRNN : ë§¨ì•žì—
Glove : í•©ì³ì„œ ê°€ëŠ¥?

-------------
engineering : ëª¨ë¸ì„ ì ìš©í•´ì„œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê³ ì³ê°€ëŠ” ë°©ë²• 

***ìžì—°ì–´ ì²˜ë¦¬ì— ëŒ€í•´ì„œ ë‚´ë¶€ì ì¸ main features***
1. TF 
2. ë‹¨ì–´ì™€ì˜ ê±°ë¦¬ 
3. ë‹¨ì–´ì˜ ìˆœì„œ/ë°©í–¥

ë§ˆìŠ¤í¬ ëª¨ë¸? 

***ë¬¸ì œ í•´ê²° ë°©ë²•***

1. ì½ì–´ë³´ê³ , ë¼ë²¨ë§ì´ ê°€ëŠ¥í•œê°€? ì–¸ì–´ë¥¼ ì ìš©í• ìˆ˜ ìžˆëŠ”ì§€?
2. Bertë¡œ ê°€ëŠ”ì§€? ì•„ë‹˜ êµ¬ì¡°ì ìœ¼ë¡œ ê°€ì•¼í•˜ëŠ”ì§€? 
ìŠ¹ë¶€ëŠ” ì „ì²˜ë¦¬ì—ì„œ ë‚˜ì˜¨ë‹¤.
Hyperparameter : AutoML, optuna 
ì•½ì–´ê°™ì€ ê²½ìš°ëŠ” ì–´ë–¤ì‹ìœ¼ë¡œ? Bertì—ì„œ ìž˜ëª»ì¸ì‹í• ìˆ˜ ìžˆì„ë“¯
ë‹¨ì–´ë¥¼ ê²¹ì¹˜ê²Œ í•˜ë©´ 
ë§ˆí‚¹? ë§ˆìŠ¤í‚¹ì„ ê±¸ì–´ë†“ìŒ 
ë² ì´ìŠ¤ë¼ì¸ì„ í•˜ê³  ì˜¤ë‹µì„ ë³´ë©´ì„œ ë§žì¶°ê°€ë©´ì„œ ì´í•´í•¨. 

Bertì˜ í† í°ì œí•œí™” 

///// ì˜ê²¬ 
ì˜ˆë¡œ Bertì˜ í•œê³„ê°€ ì¡´ìž¬í•œë‹¤ë©´, 
ì´ëŸ° ë¬¸ì œì˜€ì„ë•ŒëŠ” ë‹¤ë¥¸ ê²ƒ ì‚¬ìš© í•  ê²ƒ
ë‹¤ë¥¸ ì ‘ê·¼ë²•: ê° ë°©ë²•ì˜ ì•½ì ì„ í™•ì¸í•˜ë©´ ì¢‹ì„ ë“¯
Bert (Transformer)ë§Œ ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ, ê¸°ì¡´ ëª¨ë¸ë“¤ RNN
A : architecture 




-------

ìš°ìŠ¹ìž ë°©ë²•ì— ëŒ€í•œ ì„¤ëª… 
ìš°ë¦¬ê°€ ì–¸ê¸‰í–ˆë˜ ë¶€ë¶„ì— ëŒ€í•´ì„œ ê°ìž ëŒë ¤ë³´ê³  ë°œí‘œ 





-----------------------------------------------------------
ìŒì„±ì¸ì‹ legacy model 
ìŒì„± ì‹ í˜¸ë¥¼ fftë¡œ ë³€í™˜í•˜ê³  mel spectrogram ìœ¼ë¡œ image ë½‘ì€ ë‹¤ìŒì— ê·¸ ì´ë¯¸ì§€ë¡œ ê°ì„±ë¶„ë¥˜
nlpì—ì„œ ìŒì„±ì¸ì‹ì€ ìž‘ì€ ë¶„íŒŒì´ê³ , signal processing ì´ë©´ ìŒì„±ì¸ì‹ìª½ì´ ë” ë§žìŒ
NLPëŠ” model architecture ìª½ì— ë” ê´€ì‹¬ì´ ë§ŽìŒ
ASR (Automated Speech Recognition) 

HMM (Hidden Markov Model) +GMM (Gaussian Mixture Model) -> RNN+GMM -> RNN -> Transformer -> Wav2Vec
RNN-> Transformer 

http://speech.cbnu.ac.kr/

NLP : ELMO -> BERT -> GPT2 -> GPT3

