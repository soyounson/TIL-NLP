# ğŸ¦– Quora Question Pairs (QQP)

**written by Soyoun Son**         
**Date : 051722**


#### ğŸ¦† Kaggle ref : https://www.kaggle.com/c/quora-question-pairs

## ğŸŒ± í’€ììŠ¤ì¿¨ (5ì›” 17ì¼ 2022ë…„, ëª¨ë‘ì—°êµ¬ì†Œ ìº í¼ìŠ¤) 

![Fig00](/image/google_bert.png)


## ğŸ»â€â„ï¸ ë°œí‘œ 1 
- baseline training ì´í›„ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œ EDA ì‘ì—… ì§„í–‰ 
- **[lime](https://github.com/marcotcr/lime) library : DLì—ì„œ ì œëŒ€ë¡œ í•™ìŠµí•˜ê³  ìˆëŠ”ì§€ë¥¼ NLPì— ì ìš©í•¨ **
(ref: https://c3.ai/glossary/data-science/lime-local-interpretable-model-agnostic-explanations/) 
(ref2 : https://christophm.github.io/interpretable-ml-book/lime.html)
 
## ğŸ»â€â„ï¸ ë°œí‘œ 2 
### â˜ºï¸ ë‹¨ì–´í‘œí˜„
ì½”í¼ìŠ¤ êµ¬ì„±í•˜ê³  id ì£¼ëŠ” ê²ƒ 
### í†µê³„ê¸°ë°˜ ê¸°ë²• : ë‹¨ì–´ ë¹ˆë„ìˆ˜ì— ë”°ë¼ ë²¡í„°ë¡œ í‘œí˜„ TF-IDF, ì½”ì‚¬ì¸ ìœ ì‚¬ë„, 
ì ë³„ìƒí˜¸ì •ë³´ëŸ‰ (pointwise mutual information) 
### ì¶”ë¡ ê¸°ë°˜ê¸°ë²• : ë¶„í¬ê°€ì„¤ì— ê¸°ì´ˆë¥¼ í•˜ì§€ë§Œ ë¬¸ì¥ë‚´ì—ì„œ ì§€ì—½ì ì¸ ì˜ì—­, CBOW, skip-gram-> ë‘ê°œ ë¬¶ì–´ì„œ word2vec

- CBOW 
- word2vec : ë§ë­‰ì¹˜ê³„ìˆ˜ê°€ ë§ì•„ì§€ë©´ ë³µì¡ë„ê°€ ì»¤ì§ -> ì†ë„ê°œì„  ë°©ë²•ìœ¼ë¡œ embedding, negative sampling (sigmoid fnì´ìš©í•´ì„œ ì´ì§„ë¶„ë¥˜) 

### â˜ºï¸ [GloVe : Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)
í†µê³„ê¸°ë°˜í‘œí˜„ë°©ì‹ + ì˜ë¯¸ì¶”ë¡  
extract global corpus statistics information
ë™ì‹œë°œìƒí™•ë¥ ì— ëŒ€í•œ ë¹„ìœ¨ì„ êµ¬í•˜ëŠ” ê²ƒì´ ë” ì¤‘ìš”í•¨ì„ ë‚˜íƒ€ëƒ„. ë¹„ìœ¨ì— ë”°ë¼ì„œ ì¸ì§€í–ˆì„ë•Œ ì¢€ ë” ê°€ì‹œì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì„ 
ì„ë² ë”©ëœ ì¤‘ì‹¬ë‹¨ì–´ì™€ ì£¼ë³€ë‹¨ì–´ ë²¡í„°ì˜ ë‚´ì ì´ ì „ì²´ ì½”í¼ìŠ¤ì—ì„œì˜ ë™ì‹œ ë“±ì¥í™•ë¥ ì´ ë˜ë„ë¡ ë§Œë“œëŠ” ê²ƒ 
ì •ë³´ëŸ‰ì— ë”°ë¼ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ 

### â˜ºï¸ embedding layer 
ì´ì‚°ì ì¸ ê°’ë“±ì„ ìˆ˜ì¹˜ì ì¸ ê°’ìœ¼ë¡œ ë§µí•‘ì‹œí‚¤ëŠ” ê²ƒ 
https://www.kaggle.com/code/rajmehra03/a-detailed-explanation-of-keras-embedding-layer/notebook
[next sentend prediction, NSP](https://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f)

### Bert ì™€ ë‹¨ì–´ ì„ë² ë”© 
#### input representation 
- token embedding 
- segment embedding 
- position embedding 
-> ëª¨ë‘ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ ê°–ê³ ì™€ì„œ ê³„ì‚° 

#### ë‚´ë¶€ì½”ë“œ
tf.gradients()ë¡œ í•™ìŠµ 


## ğŸ»â€â„ï¸ ë°œí‘œ 3, [NLP ë¬¸ì œí•´ê²°ì „ëµ](https://www.notion.so/modulabs/NLP-bdc7562bc0e146c69cbf55cf9590dcf7)
#### process 
understand problem -> EDA -> baseline models -> improve performance 

### ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œ
- ë¬¸ì¥ì„ ì´í•´í•˜ë©´ í’€ìˆ˜ìˆëŠ” ë¬¸ì œë“¤ì´ ìƒë‹¹ìˆ˜ë¥¼ ì°¨ì§€í•¨. ê·¸ë˜ì„œ ì´ê²ƒì¸ì§€ë¥¼ í™•ì¸í•˜ê³  ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ê°. 
- ìˆ˜ì¹˜í™”ë¡œ ë‚˜íƒ€ë‚´ì•¼í•¨
- ë¬¸ì¥ì„ ì´í•´í•˜ë©´ í’€ìˆ˜ìˆëŠ” ë¬¸ì œë“¤ì´ ìƒë‹¹ìˆ˜ë¥¼ ì°¨ì§€í•¨. ê·¸ë˜ì„œ ì´ê²ƒì¸ì§€ë¥¼ í™•ì¸í•˜ê³  ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ê°. 
- ê·¸ë¦¬ê³  ë‚˜ì„œ ë² ì´ìŠ¤ë¼ì¸ëŒ€ë¹„ ì–´ë–¤ì‹ìœ¼ë¡œ ìµœì í™” í• ê²ƒì¸ì§€ ìƒê°í•´ ë³¼ ê²ƒ 
- ìµœì í™”ëŠ” ëŒ€ì²´ì ìœ¼ë¡œ íŒ¨í„´ì´ ìˆìŒ. 
- out of the box (creative) ì¸ ê²½ìš°ëŠ” ë…¼ë¬¸ì„ ì‘ì„±í•˜ëŠ” ê²ƒ (1ë…„ì´ìƒ ì‹œê°„ì†Œìš”) ë”°ë¼ì„œ ìºê¸€ê³¼ëŠ” ê±°ë¦¬ê°€ ìˆìŒ. 
- NLU : ê¸€ë£¨ì— 8ê°œ ìŠˆí¼ê¸€ë£¨ì— 9ê°œ? ì´ëŸ°ì‹ìœ¼ë¡œ?
 - [wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) : ëª¨ë“  ë‹¨ì–´ë“¤ì„ ì •í˜•í™” ì‹œì¼œë†“ìŒ 
- NLG : ë²ˆì—­, ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒ 
  - QQP : ìœ ì‚¬ë„ì˜ˆì¸¡ ë¬¸ì œì— í•´ë‹¹í•¨ 

- [papers w/ code](https://paperswithcode.com/)
#### GLUE 
9ê°œì˜ ìœ í˜•ìœ¼ë¡œ ë§Œë“¤ì–´ ë†“ìŒ 
#### Super GLUE
- ê¸°ì—…ë“¤ì´ í•¨ê»˜ ë§Œë“  ê²ƒ. 8ê°œì˜ ìœ í˜•ìœ¼ë¡œ ë§Œë“¤ì–´ ë†“ìŒ 
- êµ¬ë¶„ì€ ë°ì´í„°ì…‹ì˜ ì¶œì²˜, í¬ê¸°, ë¼ë²¨ë“±ì— ë”°ë¼ì„œ ë˜ì–´ ìˆìŒ

### ìì—°ì–´ì˜ ê²½ìš°
1. ì§€ì‹ì´ìš© -> domain knowledgeê°€ í•„ìš”
2. ì˜ë¯¸ì´í•´ì´ìš© 
3. ì„ë² ë”© ìˆ˜ì¤€ : ë¬¸ìê°€ ë“¤ì–´ì˜¤ë©´ ê·¸ê²ƒì´ ë²¡í„°ê°€ ë˜ëŠ” ê³¼ì •ì´ë‚˜ ê²°ê³¼ë¬¼ 
  (ë‹¨ì–´-> ë¬¸ì¥ -> ë¬¸ì„œ), pretrained modelì—ì„œ bertê°€ ë‚˜ì˜´ 
  ì´ê²ƒì„ ì–´ë– í•œ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒ [EEAP](https://explosion.ai/blog/deep-learning-formula-nlp#embed)
  NLPëª¨ë¸ì— ëŒ€í•œ SOTAê³µì‹
  ê°€ì¥ dependentí•œ ê²ƒì´ í† í¬ë‚˜ì´ì €? í† í¬ë‚˜ì´ì €ê°€ ë¨¼ì € í•™ìŠµì„ í•˜ê³ 
  0ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ í† í¬ë‚˜ì´ì € í•˜ëŠ” ê²ƒìœ¼ë¡œì¨ ë²„íŠ¸ê°€ ?
  a. ì„ë² ë”© : fixed sizeê°€ í‚¤ ì„. í˜„ì¬ìˆ˜ì¤€ì—ì„œëŠ” ì„ë² ë”©ì´ ë‹¤ í•„ìš”í•˜ë‹¤.
  b. ì¸ì½”ë“œ 
[Roberta: a robustly optimized bert pretraining approach](https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.md) 
LSTMì„ ë„£ëŠ” ê²ƒì´ í•˜ë‚˜ì˜ íŒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ. 

![Fig01](/image/Untitled.png)

 c. attend : dimensionì´ í•˜ë‚˜ ì¤„ì–´ì•¼ í•¨. clsí†µí•´ì„œ ë½‘ëŠ” ê²ƒ ë³´ë‹¤ ë” ë†’ì€ ì ìˆ˜ë¥¼ ì¤„ìˆ˜ë„ ìˆìŒ. 
 d. predict: FFNN -> relu, gelu, tanh -> normalization -> dropout (classificationë„£ê¸° ë°”ë¡œ ì „ì—)
    ê·¸ë¦¬ê³  [optuna](https://optuna.org/)ê°€ ì°¾ë„ë¡ í•¨ 
    ë‹¨ì–´ë“¤ê³¼ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ idì •ë³´, positioní•œ ì •ë³´ê°€ ê°™ì´ ë“¤ì–´ê° 
    
#### out of the box : someth creative  
papers w/ code    
ì•ë‹¨ì„ ë°”ê¾¸ë©´ ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ìˆ˜ê°€ ìˆë‹¤. í˜¹ì€ ë§ì³ì§€ê±°ë‚˜?!
T5 : encoder-decoder 
spanbert : í† í°ì„ ì´ì–´ ë¶™ì¸ ë¦¬ìŠ¤íŠ¸? í† í°? ì´ê²ƒì„ ë§ˆìŠ¤í¬ë¡œ ì‚¬ìš©í•¨ 
deberta : bertì˜ í›„ì†ì¸ë° 3ê°€ì§€ì •ë„ì˜ í° ì•„ì´ë””ì–´ë¥¼ ë„£ì–´ì„œ ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œ í•´ê²°í•¨ 
token : ë–¨ì–´ì§„ ìµœì†Œ ë‹¨ìœ„
ì‹¤ì œ ë¹„ì§€ë‹ˆìŠ¤ì—ì„œëŠ” íŒŒì¸íŠœë‹

#### extra 
CLS: stands for classification. It is added at the beginning because the training tasks here is sentence classification. And because they need an input that can represent the meaning of the entire sentence, they introduce a new tag.

#### í™•ì¸í•´ë³¼ ë‚´ìš© 
LIME, OPTUNA

#### Bert

![Fig02](/image/bert-sentence-pair.png)
sentence 1, sentence 2 
sepì€ ì´ë¯¸ ì˜ˆì•½ë˜ì–´ ìˆëŠ” ê²ƒ 
