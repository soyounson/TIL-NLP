# Natural Language Processing with Disaster Tweets 

ðŸ¦† https://www.kaggle.com/c/nlp-getting-started

> problem statement : You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.
(ì¦‰, íŠ¸ìœ— ë¬¸ìž¥ì„ ë³´ê³  disaster ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìž¥ì¸ì§€, ê·¸ì™€ ê´€ë ¨ëœ ë‹¨ì–´ê°€ ìžˆëŠ” ë¬¸ìž¥ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¬¸ì œ) [1,2]

- [x] Dataset
- [ ] EDA 
- [ ] Methodologies

#### â˜ºï¸Ž Dataset 
ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•¨ 

#### â˜» Data Files [1]
+ train.csv : the training set
+ test.csv : the test set
+ sample_submission.csv : a sample submission file in the correct format

#### â˜ºï¸Ž dataset [1]
ë°ì´í„° ì…‹ì˜ columnsì€ ì•„ëž˜ì™€ ê°™ê³ , 

+ id : a unique identifier for each tweet
+ text : the text of the tweet
+ location : the location the tweet was sent from (may be blank)
+ keyword : a particular keyword from the tweet (may be blank)
+ target : in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)

ê° íŒŒì¼ë³„ ì°¨ì´ê°€ ì¡´ìž¬í•˜ëŠ”ë° test.csvì˜ ê²½ìš° 4ê°œì˜ columnsì´ ì¡´ìž¬í•˜ê³ , train.csvì˜ ê²½ìš° label (target) ê¹Œì§€ ì´ 5ê°œì˜ columnsì´ ì¡´ìž¬

#### â˜ºï¸Ž EDA [3]
EDAëŠ” targetì´ 1ê³¼ 0ìœ¼ë¡œ ë‚˜ëˆ ì„œ data distribution ë° visualizationì„ ì§„í–‰í•˜ì˜€ë‹¤. 

+ number of characters, words in tweets
+ average word lenght in a tweet
+ common stopwords in tweets
+ analyzing punctuations 
+ common words? : ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë§Žì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ 
+ Ngram analysis : do a bigram (n=2) analysis over the tweets
```
vec = CountVectorizer(ngram_rang




#### â˜ºï¸Ž Methodologies




### ref 
[1] https://www.kaggle.com/competitions/nlp-getting-started/data
[2] https://onground-korea.github.io/machine_learning/2021/03/07/Kaggle-NLP.html
[3] https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove




ìŒì„±ì¸ì‹ legacy model 
ìŒì„± ì‹ í˜¸ë¥¼ fftë¡œ ë³€í™˜í•˜ê³  mel spectrogram ìœ¼ë¡œ image ë½‘ì€ ë‹¤ìŒì— ê·¸ ì´ë¯¸ì§€ë¡œ ê°ì„±ë¶„ë¥˜
nlpì—ì„œ ìŒì„±ì¸ì‹ì€ ìž‘ì€ ë¶„íŒŒì´ê³ , signal processing ì´ë©´ ìŒì„±ì¸ì‹ìª½ì´ ë” ë§žìŒ
NLPëŠ” model architecture ìª½ì— ë” ê´€ì‹¬ì´ ë§ŽìŒ
ASR (Automated Speech Recognition) 

HMM (Hidden Markov Model) +GMM (Gaussian Mixture Model) -> RNN+GMM -> RNN -> Transformer -> Wav2Vec
RNN-> Transformer 

http://speech.cbnu.ac.kr/

NLP : ELMO -> BERT -> GPT2 -> GPT3
