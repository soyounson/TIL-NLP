# Natural Language Processing with Disaster Tweets 

ðŸ¦† https://www.kaggle.com/c/nlp-getting-started

> problem statement : You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.
(ì¦‰, íŠ¸ìœ— ë¬¸ìž¥ì„ ë³´ê³  disaster ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìž¥ì¸ì§€, ê·¸ì™€ ê´€ë ¨ëœ ë‹¨ì–´ê°€ ìžˆëŠ” ë¬¸ìž¥ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¬¸ì œ) [1,2]

- [x] Dataset
- [x] EDA 
- [x] Methodologies

### â˜ºï¸Ž Dataset 
ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•¨ 

#### â˜» Data Files [1]
+ train.csv : the training set
+ test.csv : the test set
+ sample_submission.csv : a sample submission file in the correct format

#### â˜» dataset [1]
ë°ì´í„° ì…‹ì˜ columnsì€ ì•„ëž˜ì™€ ê°™ê³ , 

+ id : a unique identifier for each tweet
+ text : the text of the tweet
+ location : the location the tweet was sent from (may be blank)
+ keyword : a particular keyword from the tweet (may be blank)
+ target : in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)

ê° íŒŒì¼ë³„ ì°¨ì´ê°€ ì¡´ìž¬í•˜ëŠ”ë° test.csvì˜ ê²½ìš° 4ê°œì˜ columnsì´ ì¡´ìž¬í•˜ê³ , train.csvì˜ ê²½ìš° label (target) ê¹Œì§€ ì´ 5ê°œì˜ columnsì´ ì¡´ìž¬

#### â˜» Exploratory Data Analysis, EDA [3]
EDAëŠ” targetì´ 1ê³¼ 0ìœ¼ë¡œ ë‚˜ëˆ ì„œ data distribution ë° visualizationì„ ì§„í–‰í•˜ì˜€ë‹¤. 

+ number of characters, words in tweets
+ average word lenght in a tweet
+ common stopwords in tweets
+ analyzing punctuations 
+ common words? : ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë§Žì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ 
+ Ngram analysis : do a bigram (n=2) analysis over the tweets
```
vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
bag_of_words = vec.transform(corpus)
```
#### â˜» Data cleaning 
basic cleaning such as **spelling correction,removing punctuations,removing html tags and emojis etc.***
+ removing urls
+ removing HTML tags
+ removing Emojis
+ removing punctuations
+ spelling correction 

#### â˜» GloVe for Vecttorization [4]
use GloVe pretrained corpus model to represent our words.It is available in 3 varieties 

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ìž„ë² ë”© ë°©ë²•ë¡ ìž…ë‹ˆë‹¤. ì•žì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ìž¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëžŒì§í•©ë‹ˆë‹¤.

### â˜ºï¸Ž Baseline Methodologies

embedding + LSTM ëª¨ë¸ ë§Œë“¤ê³ ,  
```
model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

optimzer=Adam(learning_rate=1e-5)
model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])
```
ë°ì´í„°ë¥¼ train/testë¡œ ë‚˜ëˆ„ê³ , training ë° predictionê¹Œì§€ ì§„í–‰í•¨. 

```
X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)

history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)
y_pre=model.predict(test)
```

### ref 
[1] https://www.kaggle.com/competitions/nlp-getting-started/data

[2] https://onground-korea.github.io/machine_learning/2021/03/07/Kaggle-NLP.html

[3] https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove

[4] https://wikidocs.net/22885






-----------------------------------------------------------
ìŒì„±ì¸ì‹ legacy model 
ìŒì„± ì‹ í˜¸ë¥¼ fftë¡œ ë³€í™˜í•˜ê³  mel spectrogram ìœ¼ë¡œ image ë½‘ì€ ë‹¤ìŒì— ê·¸ ì´ë¯¸ì§€ë¡œ ê°ì„±ë¶„ë¥˜
nlpì—ì„œ ìŒì„±ì¸ì‹ì€ ìž‘ì€ ë¶„íŒŒì´ê³ , signal processing ì´ë©´ ìŒì„±ì¸ì‹ìª½ì´ ë” ë§žìŒ
NLPëŠ” model architecture ìª½ì— ë” ê´€ì‹¬ì´ ë§ŽìŒ
ASR (Automated Speech Recognition) 

HMM (Hidden Markov Model) +GMM (Gaussian Mixture Model) -> RNN+GMM -> RNN -> Transformer -> Wav2Vec
RNN-> Transformer 

http://speech.cbnu.ac.kr/

NLP : ELMO -> BERT -> GPT2 -> GPT3
