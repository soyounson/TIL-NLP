# Natural Language Processing with Disaster Tweets 

ðŸ¦† https://www.kaggle.com/c/nlp-getting-started

> problem statement : You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.
(ì¦‰, íŠ¸ìœ— ë¬¸ìž¥ì„ ë³´ê³  disaster ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìž¥ì¸ì§€, ê·¸ì™€ ê´€ë ¨ëœ ë‹¨ì–´ê°€ ìžˆëŠ” ë¬¸ìž¥ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¬¸ì œ) [1,2]

- [x] Dataset
- [x] EDA 
- [x] Methodologies

### â˜ºï¸Ž Dataset 
ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•¨ 

#### â˜» Data Files [1]
+ train.csv : the training set
+ test.csv : the test set
+ sample_submission.csv : a sample submission file in the correct format

#### â˜» dataset [1]
ë°ì´í„° ì…‹ì˜ columnsì€ ì•„ëž˜ì™€ ê°™ê³ , 

+ id : a unique identifier for each tweet
+ text : the text of the tweet
+ location : the location the tweet was sent from (may be blank)
+ keyword : a particular keyword from the tweet (may be blank)
+ target : in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)

ê° íŒŒì¼ë³„ ì°¨ì´ê°€ ì¡´ìž¬í•˜ëŠ”ë° test.csvì˜ ê²½ìš° 4ê°œì˜ columnsì´ ì¡´ìž¬í•˜ê³ , train.csvì˜ ê²½ìš° label (target) ê¹Œì§€ ì´ 5ê°œì˜ columnsì´ ì¡´ìž¬

> id, keyword, text ê°€ ì¤‘ìš”í•œ ê²ƒ 

#### â˜» Exploratory Data Analysis, EDA [3]
EDAëŠ” targetì´ 1ê³¼ 0ìœ¼ë¡œ ë‚˜ëˆ ì„œ data distribution ë° visualizationì„ ì§„í–‰í•˜ì˜€ë‹¤. 

+ number of characters, words in tweets
+ average word lenght in a tweet
+ common stopwords in tweets
+ analyzing punctuations 
+ common words? : ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë§Žì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ 
+ Ngram analysis : do a bigram (n=2) analysis over the tweets
```
vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
bag_of_words = vec.transform(corpus)
```
#### â˜» Data cleaning 
basic cleaning such as **spelling correction,removing punctuations,removing html tags and emojis etc.***
+ removing urls
+ removing HTML tags
+ removing Emojis
+ removing punctuations
+ spelling correction 

#### â˜» GloVe for Vecttorization [4]
use GloVe pretrained corpus model to represent our words.It is available in 3 varieties 

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ìž„ë² ë”© ë°©ë²•ë¡ ìž…ë‹ˆë‹¤. ì•žì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ìž¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëžŒì§í•©ë‹ˆë‹¤.

### â˜ºï¸Ž Baseline Methodologies

embedding + LSTM ëª¨ë¸ ë§Œë“¤ê³ ,  
```
model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

optimzer=Adam(learning_rate=1e-5)
model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])
```
ë°ì´í„°ë¥¼ train/testë¡œ ë‚˜ëˆ„ê³ , training ë° predictionê¹Œì§€ ì§„í–‰í•¨. 

```
X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)

history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)
y_pre=model.predict(test)
```

### ref 
[1] https://www.kaggle.com/competitions/nlp-getting-started/data

[2] https://onground-korea.github.io/machine_learning/2021/03/07/Kaggle-NLP.html

[3] https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove

[4] https://wikidocs.net/22885

--------------------------------------------------------------
## ðŸŒ± í’€ìžŽìŠ¤ì¿¨ (5ì›” 2ì¼) 

[NLP preprocess](https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z/notebook) 

1. read and explore data 
2. text cleaning 
capitalization : ëŒ€ë¬¸ìž-> ì†Œë¬¸ìž 
expand the contractions
remove noise, punctuations
íŠ¹ìˆ˜ë¬¸ìžëŠ” characterë¡œ ë°”ê¿ˆ 
í•œêµ­ì–´ë¡œ ìž¡ì—…í• ë•ŒëŠ” capitalization, expand the contractionsë¶€ë¶„ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ 
3. text pre-processing 
+ tokenization 
+ remove stop words : nltkì— tokenizer ì‚¬ì „ì´ ì¡´ìž¬. í•œêµ­ì–´ì˜ ê²½ìš°ëŠ” POS tagging ì´í›„ì— ì§„í–‰
+ stemming : ì–´ê°„ì¶”ì¶œ, ë¶„ì„ê²°ê³¼ê°€ ì•ˆì¢‹ì•˜ì„ë•Œ í•˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì  
+ POS tagging : í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ. ë™ì‚¬/ëª…ì‚¬/... ë¬¸ë§¥ì ìœ¼ë¡œ ìž˜ íŒŒì•…í• ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ê³¼ì •. í’ˆì‚¬ì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë°œìƒ. í•œêµ­ì–´ì˜ ê²½ìš° KoNLPy
+ Lemmatization : í‘œì œì–´ ì¶”ì¶œ. ê¸°ë³¸í˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì •. ë”°ë¼ì„œ í’ˆì‚¬ ì§€ì •í•˜ëŠ” PoS taggingí•œ í›„ì— ì§„í–‰í•´ì•¼ í•¨. 
+ (optional) language detection 

í•œêµ­ì–´ì˜ ê²½ìš° í† í°í™”-> POS tagging -> stemming/lemmatization -> remove stop words
í•œêµ­ì–´ì˜ ê²½ìš° í‘œì œì–´ ì¶”ì¶œ, ì–´ê°„ì¶”ì¶œì˜ ì°¨ì´ê°€ ë§Žì´ ì—†ê³ , ì–´ê°„ì¶”ì¶œì´ ê·¸ë‚˜ë§ˆ ë” ë§Žì´ ì‚¬ìš©ë¨

4. Text Feature Extraction 
(1) weighted words - BOW
- countvectorizer
- TF-IDF
ì¹´ìš´íŒ…í•˜ì§€ë§Œ, ìœ„ì¹˜ì •ë³´ë¥¼ ìžƒìŒ. ê·¸ëž˜ì„œ ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•… ë¶ˆê°€ 
(2) word embedding 
- word2vec   -> gensim 
- Glove
- FastText
- Bert : transformer 30ì–µê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í•™ìŠµì‹œí‚¨í›„, ë³¸ì¸ì˜ í…ŒìŠ¤í¬ì— ë”°ë¼ì„œ fine tunningì„ í•˜ë©´ ì •í™•ë„ê°€ ë†’ìŒ. 
(3) comparison of feature extraction techinique  

-> ì–´ë–¤ ì˜ë¯¸ìžˆëŠ” featureì„ ë½‘ì•„ë‚¼ìˆ˜ ìžˆì„ê¹Œ? 























-----------------------------------------------------------
ìŒì„±ì¸ì‹ legacy model 
ìŒì„± ì‹ í˜¸ë¥¼ fftë¡œ ë³€í™˜í•˜ê³  mel spectrogram ìœ¼ë¡œ image ë½‘ì€ ë‹¤ìŒì— ê·¸ ì´ë¯¸ì§€ë¡œ ê°ì„±ë¶„ë¥˜
nlpì—ì„œ ìŒì„±ì¸ì‹ì€ ìž‘ì€ ë¶„íŒŒì´ê³ , signal processing ì´ë©´ ìŒì„±ì¸ì‹ìª½ì´ ë” ë§žìŒ
NLPëŠ” model architecture ìª½ì— ë” ê´€ì‹¬ì´ ë§ŽìŒ
ASR (Automated Speech Recognition) 

HMM (Hidden Markov Model) +GMM (Gaussian Mixture Model) -> RNN+GMM -> RNN -> Transformer -> Wav2Vec
RNN-> Transformer 

http://speech.cbnu.ac.kr/

NLP : ELMO -> BERT -> GPT2 -> GPT3
