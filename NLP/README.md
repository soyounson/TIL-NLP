# Natural Language Processing with Disaster Tweets 

ðŸ¦† https://www.kaggle.com/c/nlp-getting-started

> problem statement : You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.
(ì¦‰, íŠ¸ìœ— ë¬¸ìž¥ì„ ë³´ê³  disaster ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìž¥ì¸ì§€, ê·¸ì™€ ê´€ë ¨ëœ ë‹¨ì–´ê°€ ìžˆëŠ” ë¬¸ìž¥ì¸ì§€ íŒë‹¨í•˜ëŠ” ë¬¸ì œ) [1,2]

- [x] Dataset
- [x] EDA 
- [x] Methodologies

### â˜ºï¸Ž Dataset 
ë°ì´í„°ì…‹ì— ëŒ€í•œ ì´í•´ê°€ í•„ìš”í•¨ 

#### â˜» Data Files [1]
+ train.csv : the training set
+ test.csv : the test set
+ sample_submission.csv : a sample submission file in the correct format

#### â˜» dataset [1]
ë°ì´í„° ì…‹ì˜ columnsì€ ì•„ëž˜ì™€ ê°™ê³ , 

+ id : a unique identifier for each tweet
+ text : the text of the tweet
+ location : the location the tweet was sent from (may be blank)
+ keyword : a particular keyword from the tweet (may be blank)
+ target : in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)

ê° íŒŒì¼ë³„ ì°¨ì´ê°€ ì¡´ìž¬í•˜ëŠ”ë° test.csvì˜ ê²½ìš° 4ê°œì˜ columnsì´ ì¡´ìž¬í•˜ê³ , train.csvì˜ ê²½ìš° label (target) ê¹Œì§€ ì´ 5ê°œì˜ columnsì´ ì¡´ìž¬

> id, keyword, text ê°€ ì¤‘ìš”í•œ ê²ƒ 

#### â˜» Exploratory Data Analysis, EDA [3]
EDAëŠ” targetì´ 1ê³¼ 0ìœ¼ë¡œ ë‚˜ëˆ ì„œ data distribution ë° visualizationì„ ì§„í–‰í•˜ì˜€ë‹¤. 

+ number of characters, words in tweets
+ average word lenght in a tweet
+ common stopwords in tweets
+ analyzing punctuations 
+ common words? : ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ë§Žì´ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸ 
+ Ngram analysis : do a bigram (n=2) analysis over the tweets
```
vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
bag_of_words = vec.transform(corpus)
```
#### â˜» Data cleaning 
basic cleaning such as **spelling correction,removing punctuations,removing html tags and emojis etc.***
+ removing urls
+ removing HTML tags
+ removing Emojis
+ removing punctuations
+ spelling correction 

#### â˜» GloVe for Vecttorization [4]
use GloVe pretrained corpus model to represent our words.It is available in 3 varieties 

ê¸€ë¡œë¸Œ(Global Vectors for Word Representation, GloVe)ëŠ” ì¹´ìš´íŠ¸ ê¸°ë°˜ê³¼ ì˜ˆì¸¡ ê¸°ë°˜ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ 2014ë…„ì— ë¯¸êµ­ ìŠ¤íƒ í¬ë“œëŒ€í•™ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ìž„ë² ë”© ë°©ë²•ë¡ ìž…ë‹ˆë‹¤. ì•žì„œ í•™ìŠµí•˜ì˜€ë˜ ê¸°ì¡´ì˜ ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ LSA(Latent Semantic Analysis)ì™€ ì˜ˆì¸¡ ê¸°ë°˜ì˜ Word2Vecì˜ ë‹¨ì ì„ ì§€ì í•˜ë©° ì´ë¥¼ ë³´ì™„í•œë‹¤ëŠ” ëª©ì ìœ¼ë¡œ ë‚˜ì™”ê³ , ì‹¤ì œë¡œë„ Word2Vecë§Œí¼ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. í˜„ìž¬ê¹Œì§€ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´ ë‹¨ì •ì ìœ¼ë¡œ Word2Vecì™€ GloVe ì¤‘ì—ì„œ ì–´ë–¤ ê²ƒì´ ë” ë›°ì–´ë‚˜ë‹¤ê³  ë§í•  ìˆ˜ëŠ” ì—†ê³ , ì´ ë‘ ê°€ì§€ ì „ë¶€ë¥¼ ì‚¬ìš©í•´ë³´ê³  ì„±ëŠ¥ì´ ë” ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ëžŒì§í•©ë‹ˆë‹¤.

### â˜ºï¸Ž Baseline Methodologies

embedding + LSTM ëª¨ë¸ ë§Œë“¤ê³ ,  
```
model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

optimzer=Adam(learning_rate=1e-5)
model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])
```
ë°ì´í„°ë¥¼ train/testë¡œ ë‚˜ëˆ„ê³ , training ë° predictionê¹Œì§€ ì§„í–‰í•¨. 

```
X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)

history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)
y_pre=model.predict(test)
```

### ref 
[1] https://www.kaggle.com/competitions/nlp-getting-started/data

[2] https://onground-korea.github.io/machine_learning/2021/03/07/Kaggle-NLP.html

[3] https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove

[4] https://wikidocs.net/22885

--------------------------------------------------------------
## ðŸŒ± í’€ìžŽìŠ¤ì¿¨ (5ì›” 2ì¼) 

[NLP preprocess](https://www.kaggle.com/code/longtng/nlp-preprocessing-feature-extraction-methods-a-z/notebook) 

1. read and explore data 
2. text cleaning 
capitalization : ëŒ€ë¬¸ìž-> ì†Œë¬¸ìž 
expand the contractions
remove noise, punctuations
íŠ¹ìˆ˜ë¬¸ìžëŠ” characterë¡œ ë°”ê¿ˆ 
í•œêµ­ì–´ë¡œ ìž¡ì—…í• ë•ŒëŠ” capitalization, expand the contractionsë¶€ë¶„ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ 
3. text pre-processing 
+ tokenization 
+ remove stop words : nltkì— tokenizer ì‚¬ì „ì´ ì¡´ìž¬. í•œêµ­ì–´ì˜ ê²½ìš°ëŠ” POS tagging ì´í›„ì— ì§„í–‰
+ stemming : ì–´ê°„ì¶”ì¶œ, ë¶„ì„ê²°ê³¼ê°€ ì•ˆì¢‹ì•˜ì„ë•Œ í•˜ëŠ” ê²½ìš°ê°€ ì¼ë°˜ì  
+ POS tagging : í’ˆì‚¬ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ. ë™ì‚¬/ëª…ì‚¬/... ë¬¸ë§¥ì ìœ¼ë¡œ ìž˜ íŒŒì•…í• ìˆ˜ ìžˆë„ë¡ í•˜ëŠ” ê³¼ì •. í’ˆì‚¬ì— ë”°ë¼ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ëŠ” ê²½ìš°ê°€ ë°œìƒ. í•œêµ­ì–´ì˜ ê²½ìš° KoNLPy
+ Lemmatization : í‘œì œì–´ ì¶”ì¶œ. ê¸°ë³¸í˜•ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê³¼ì •. ë”°ë¼ì„œ í’ˆì‚¬ ì§€ì •í•˜ëŠ” PoS taggingí•œ í›„ì— ì§„í–‰í•´ì•¼ í•¨. 
+ (optional) language detection 

í•œêµ­ì–´ì˜ ê²½ìš° í† í°í™”-> POS tagging -> stemming/lemmatization -> remove stop words
í•œêµ­ì–´ì˜ ê²½ìš° í‘œì œì–´ ì¶”ì¶œ, ì–´ê°„ì¶”ì¶œì˜ ì°¨ì´ê°€ ë§Žì´ ì—†ê³ , ì–´ê°„ì¶”ì¶œì´ ê·¸ë‚˜ë§ˆ ë” ë§Žì´ ì‚¬ìš©ë¨

4. Text Feature Extraction 
(1) weighted words - BOW
- countvectorizer
- TF-IDF
ì¹´ìš´íŒ…í•˜ì§€ë§Œ, ìœ„ì¹˜ì •ë³´ë¥¼ ìžƒìŒ. ê·¸ëž˜ì„œ ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•… ë¶ˆê°€ 
(2) word embedding 
- word2vec   -> gensim 
- Glove
- FastText
- Bert : transformer 30ì–µê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ í•™ìŠµì‹œí‚¨í›„, ë³¸ì¸ì˜ í…ŒìŠ¤í¬ì— ë”°ë¼ì„œ fine tunningì„ í•˜ë©´ ì •í™•ë„ê°€ ë†’ìŒ. 
(3) comparison of feature extraction techinique  

-> ì–´ë–¤ ì˜ë¯¸ìžˆëŠ” featureì„ ë½‘ì•„ë‚¼ìˆ˜ ìžˆì„ê¹Œ? 


++++++++++

[TF-IDF ê°œë…](https://www.notion.so/modulabs/NLP-w-DL-061fbb36c67d494fa062309914b4842d?p=f0a9b205d22545fe9e0707b8493f57ac) 
(1) ì „ì²˜ë¦¬ ìž‘ì—…
- ê¸ì •ë¬¸ì„œ/ë¶€ì •ë¬¸ì„œ
- tf-idf 
- ë“±ìž¥ í‚¤ì›Œë“œì— ëŒ€í•œ scoring 
(2) ëª¨ë¸ 
- decision tree, regression 
ì‹¤ë¬´ ì ìš©ì‹œ : ìž¬ë‚œì˜ ìœ ë¬´ê°€ ì•„ë‹ˆë¼ ìž¬ë‚œì˜ ì¹´í…Œê³ ë¦¬ê¹Œì§€ ì°¾ì•„ì•¼ í•œë‹¤.


Questions 

ì‹¤ë¬´ì— ì ìš©í•œë‹¤ë©´, 
- ê¸ì •/ë¶€ì •ë¼ë¦¬ ëª¨ìœ¼ëŠ” ì „ì²˜ë¦¬ : ì‚¬ëžŒì´ annotationì´ ë˜ì•¼í•¨.
- TF-IDFë§Œ ì‚¬ìš©í•œë‹¤ë©´ 2ë²ˆì„ íƒœì›Œì•¼ í•  ê²ƒ
- í‚¤ì›Œë“œ ì§€ì‹ : ì‚¬ì „êµ¬ì¶•, ì‹¤ì œ ìžì—°ì–´ì²˜ë¦¬ë¥¼ í• ë•Œ, íŠ¹ì • ë„ë©”ì¸ì— ë§žëŠ” ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ì¤Œ. ì€í–‰ ê´€ë ¨ ë„ë©”ì¸ì˜ í‚¤ì›Œë“œëŠ” 'ê³„ì¢Œì´ì²´'ì´ëŸ° ê²ƒë“¤. 
í‚¤ì›Œë“œ ì§€ì‹ì´ ì´ë¯¸ ê°–ì¶°ì§„ pre-trained modelì´ ìžˆë‹¤ë©´ 

ê³µê³µë„ë©”ì¸ ë°ì´í„°ì˜ ê²½ìš°, í‘œì¤€í™”ê°€ ëœ ë°ì´í„°ìž„. 
ì‹¤ì œ ë°ì´í„°ë¥¼ ë°›ì•˜ì„ë•ŒëŠ” ì¤„ì´ëŠ” ê²ƒì´ í•„ìš”. 

TF-IDFëŒ€ì‹  countvectorì´ìš©í•´ì„œ TFë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì„ë“¯ 
Bertì— ìµœì í™”ë˜ì–´ ìžˆëŠ” ê²ƒ 

++++++++++

[10 steps for challenging and improving NLP model](https://www.notion.so/modulabs/NLP-w-DL-061fbb36c67d494fa062309914b4842d?p=bc0c6c7c725246f9a2920c87fb2a6af3)

3. one-hot encoding : GPTê°™ì€ ê²ƒì€ ë¼ë²¨ì´ onehot encodingìœ¼ë¡œ ë‚˜ì˜´. 
4. ê¸¸ì´ê°€ ë“¤ì­‰ë‚ ì­‰í•œê²ƒ ë³´ë‹¤ íŒ¨ë”©ìžˆëŠ” ê²ƒì´ ê²°ê³¼ê°€ ì¢‹ì•˜ë‹¤?
5. ì»¤ë¸Œê°€ ë‚´ë ¤ê°€ë‹¤ê°€ ë›°ë©´ ì˜¤ë²„í”¼íŒ…í•˜ëŠ” ê²ƒì´ê³ , ê³„ì† ë‚´ë ¤ê°€ë©´ ìƒê´€ì—†ìŒ 
shuffle : ë°ì´í„°ê°€ ë­”ê°€ ì—°ì†ì ìœ¼ë¡œ ë¹„ìŠ·í•œ í˜•íƒœë¼ë©´ ê·¸ê²ƒì„ ë§‰ê¸°ìœ„í•´ì„œ ì„žëŠ” ê²ƒ
íŒŒì´í† ì¹˜ì˜ ê²½ìš° ì²«ë²ˆì§¸ ì—í¬í¬ì—ì„œ ì´ë¯¸ ê²°ì •ì´ ë‚œë‹¤. 

****** ì‹¤ì œ ì‹¤ë¬´ì—ì„œ í™•ì¸ í•  ê²ƒ ***
ëª¨ë¸ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤ ë°ì´í„°ì—ì„œ ìŠ¹ë¶€ê°€ ë‚˜ëŠ” ê²ƒ 
ì°¨ë¼ë¦¬ í† í¬ë„ˆë‚˜ì´ì €ê°™ì€ ê²ƒì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì¤‘ìš”í•¨ 
ì—­ì‚¼ê°í˜•ì´ì–´ì„œ ëª¨ë¸ì„ ë°”ê¾¸ëŠ” ê²ƒë³´ë‹¤, ì´ˆê¸°ë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì¢‹ìŒ 
ë²„íŠ¸ë„ ë²„íŠ¸ì˜ ëª¨ë¸ë³´ë‹¤ oovì¤„ì—¬ì„œ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ëŠ” ê²ƒ
í† í¬ë‚˜ì´ì €ì—ì„œ ì›Œë“œ í”¼ìŠ¤ë¥¼ ì¤„ì´ëŠ” ê²ƒ. 
íŒŒì‹±í• ë•Œ (í•œêµ­ì–´ì—ì„œ ì–´ê°„, ì–´ë¯¸ ìª¼ê°¤ë•Œ)ì™€ ê°™ì€ í† í¬ë‚˜ì´ì €ë¥¼ ì¼ì„ë•Œ ì„±ëŠ¥ ì˜¬ë¼ê°
ì—¬ì „ížˆ ë„ë©”ì¸ì´ ì¤‘ìš”. ë„ë©”ì¸ì—ì„œ ë§Žì´ ì“°ëŠ” ì–¸ì–´ê°€ ë‹¨ì–´ë¡œ í• ë‹¹ë¨?












-----------------------------------------------------------
ìŒì„±ì¸ì‹ legacy model 
ìŒì„± ì‹ í˜¸ë¥¼ fftë¡œ ë³€í™˜í•˜ê³  mel spectrogram ìœ¼ë¡œ image ë½‘ì€ ë‹¤ìŒì— ê·¸ ì´ë¯¸ì§€ë¡œ ê°ì„±ë¶„ë¥˜
nlpì—ì„œ ìŒì„±ì¸ì‹ì€ ìž‘ì€ ë¶„íŒŒì´ê³ , signal processing ì´ë©´ ìŒì„±ì¸ì‹ìª½ì´ ë” ë§žìŒ
NLPëŠ” model architecture ìª½ì— ë” ê´€ì‹¬ì´ ë§ŽìŒ
ASR (Automated Speech Recognition) 

HMM (Hidden Markov Model) +GMM (Gaussian Mixture Model) -> RNN+GMM -> RNN -> Transformer -> Wav2Vec
RNN-> Transformer 

http://speech.cbnu.ac.kr/

NLP : ELMO -> BERT -> GPT2 -> GPT3
